{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love this product\",  # Positivo\n",
    "    \"This is the worst experience ever\",  # Negativo\n",
    "    \"Absolutely fantastic work\",  # Positivo\n",
    "    \"I hate this so much\",  # Negativo\n",
    "    \"The service was excellent\",  # Positivo\n",
    "    \"This is a complete disaster\",  # Negativo\n",
    "    \"I am so happy with my purchase\",  # Positivo\n",
    "    \"This product broke after one use\",  # Negativo\n",
    "    \"Highly recommend this to everyone\",  # Positivo\n",
    "    \"I am deeply disappointed\",  # Negativo\n",
    "    \"Amazing quality and performance\",  # Positivo\n",
    "    \"The food tasted horrible\",  # Negativo\n",
    "    \"I couldn't be more satisfied\",  # Positivo\n",
    "    \"The delivery took forever\",  # Negativo\n",
    "    \"The team did a great job\",  # Positivo\n",
    "    \"The item arrived damaged\",  # Negativo\n",
    "    \"It exceeded my expectations\",  # Positivo\n",
    "    \"Worst customer service I've ever experienced\",  # Negativo\n",
    "    \"The design is stunning and practical\",  # Positivo\n",
    "    \"I regret buying this\",  # Negativo\n",
    "]\n",
    "labels = [\n",
    "    1, 0, 1, 0,  # Primeros cuatro ejemplos\n",
    "    1, 0, 1, 0,  # Nuevos ejemplos\n",
    "    1, 0, 1, 0,  # Más ejemplos\n",
    "    1, 0, 1, 0,  # Más ejemplos\n",
    "    1, 0  # Últimos dos ejemplos\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [sentence.lower().split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'this', 'product'],\n",
       " ['this', 'is', 'the', 'worst', 'experience', 'ever'],\n",
       " ['absolutely', 'fantastic', 'work'],\n",
       " ['i', 'hate', 'this', 'so', 'much'],\n",
       " ['the', 'service', 'was', 'excellent'],\n",
       " ['this', 'is', 'a', 'complete', 'disaster'],\n",
       " ['i', 'am', 'so', 'happy', 'with', 'my', 'purchase'],\n",
       " ['this', 'product', 'broke', 'after', 'one', 'use'],\n",
       " ['highly', 'recommend', 'this', 'to', 'everyone'],\n",
       " ['i', 'am', 'deeply', 'disappointed'],\n",
       " ['amazing', 'quality', 'and', 'performance'],\n",
       " ['the', 'food', 'tasted', 'horrible'],\n",
       " ['i', \"couldn't\", 'be', 'more', 'satisfied'],\n",
       " ['the', 'delivery', 'took', 'forever'],\n",
       " ['the', 'team', 'did', 'a', 'great', 'job'],\n",
       " ['the', 'item', 'arrived', 'damaged'],\n",
       " ['it', 'exceeded', 'my', 'expectations'],\n",
       " ['worst', 'customer', 'service', \"i've\", 'ever', 'experienced'],\n",
       " ['the', 'design', 'is', 'stunning', 'and', 'practical'],\n",
       " ['i', 'regret', 'buying', 'this']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_to_vector(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = [sentence_to_vector(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_sentences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(seq) for seq in vectorized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Longitud máxima de las secuencias\n",
    "max_sequence_length = max(len(seq) for seq in vectorized_sentences)\n",
    "\n",
    "# Ajustamos las secuencias para que tengan la misma longitud\n",
    "padded_sentences = pad_sequences(\n",
    "    [np.array(seq) for seq in vectorized_sentences],\n",
    "    maxlen=max_sequence_length,\n",
    "    dtype='float32',\n",
    "    padding='post'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sentences[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Flatten\n",
    "\n",
    "# Creamos el modelo\n",
    "model = Sequential([\n",
    "    SimpleRNN(64, input_shape=(max_sequence_length, 100), activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilamos el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenamos el modelo\n",
    "model.fit(padded_sentences, np.array(labels), epochs=10, batch_size=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
